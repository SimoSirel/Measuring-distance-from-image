{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "# This is needed to display the images.\n",
    "plt.rcParams['figure.figsize'] = (20, 12)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, graph):\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            # Get handles to input and output tensors\n",
    "            ops = tf.get_default_graph().get_operations()\n",
    "            all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "            tensor_dict = {}\n",
    "            for key in [\n",
    "                'num_detections', 'detection_boxes', 'detection_scores',\n",
    "                'detection_classes', 'detection_masks'\n",
    "            ]:\n",
    "                tensor_name = key + ':0'\n",
    "                if tensor_name in all_tensor_names:\n",
    "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                        tensor_name)\n",
    "            if 'detection_masks' in tensor_dict:\n",
    "                # The following processing is only for single image\n",
    "                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "                detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "                detection_masks_reframed = tf.cast(\n",
    "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "                # Follow the convention by adding back the batch dimension\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                    detection_masks_reframed, 0)\n",
    "      \n",
    "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "            # Run inference\n",
    "            output_dict = sess.run(tensor_dict,\n",
    "                                 feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "            output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "            output_dict['detection_classes'] = output_dict[\n",
    "              'detection_classes'][0].astype(np.uint8)\n",
    "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "            if 'detection_masks' in output_dict:\n",
    "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_boxes(min_score, boxes, scores, classes, categories):\n",
    "    \"\"\"Return boxes with a confidence >= `min_score`\"\"\"\n",
    "    n = len(classes)\n",
    "    idxs = []\n",
    "    for i in range(n):\n",
    "        if classes[i] in categories and scores[i] >= min_score:\n",
    "            idxs.append(i)\n",
    "\n",
    "    filtered_boxes = boxes[idxs, ...]\n",
    "    filtered_scores = scores[idxs, ...]\n",
    "    filtered_classes = classes[idxs, ...]\n",
    "    return filtered_boxes, filtered_scores, filtered_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coord(bbox, width, height):\n",
    "    \"\"\"Return boxes coordinates\"\"\"\n",
    "    xmin = bbox[1] * width\n",
    "    ymin = bbox[0] * height\n",
    "    xmax = bbox[3] * width\n",
    "    ymax = bbox[2] * height\n",
    "\n",
    "    return [xmin, ymin, xmax - xmin, ymax - ymin]\n",
    "    \n",
    "def calculate_centr(coord):\n",
    "    \"\"\"Calculate centroid for each box\"\"\"\n",
    "    return (coord[0]+(coord[2]/2), coord[1]+(coord[3]/2))\n",
    "  \n",
    "def calculate_centr_distances(centroid_1, centroid_2):\n",
    "    \"\"\"Calculate the distance between 2 centroids\"\"\"\n",
    "    return  np.sqrt((centroid_2[0]-centroid_1[0])**2 + (centroid_2[1]-centroid_1[1])**2)\n",
    "  \n",
    "def calculate_perm(centroids):\n",
    "    \"\"\"Return all combinations of centroids\"\"\"\n",
    "    permutations = []\n",
    "    for current_permutation in itertools.permutations(centroids, 2):\n",
    "        if current_permutation[::-1] not in permutations:\n",
    "            permutations.append(current_permutation)\n",
    "    return permutations\n",
    "  \n",
    "def midpoint(p1, p2):\n",
    "    \"\"\"Midpoint between 2 points\"\"\"\n",
    "    return ((p1[0] + p2[0])/2, (p1[1] + p2[1])/2)\n",
    "\n",
    "def calculate_slope(x1, y1, x2, y2):\n",
    "    \"\"\"Calculate slope\"\"\"\n",
    "    m = (y2-y1)/(x2-x1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing\n",
    "def show_image(image):\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_images(video, size = None):\n",
    "    # Params: video as VideoCapture, size=(512, 512)\n",
    "    # Yields resized images\n",
    "    # video = cv2.VideoCapture(video_filename)\n",
    "    success, image = video.read()\n",
    "    \n",
    "    while success:\n",
    "        # resize image\n",
    "        if not size:\n",
    "            image = cv2.resize(image, size, interpolation = cv2.INTER_AREA)\n",
    "        yield image\n",
    "        success, image = video.read()\n",
    "        \n",
    "    video.release();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rects(image, coordinates, color = (0, 0, 255), thickness = 5):\n",
    "    # Draws rectangles onto the image\n",
    "    # input list of Lists of  [x, y, width, height] \n",
    "    # color is tuple in BGR\n",
    "    # thickness is thickness of line in pixels\n",
    "    \n",
    "    for i in range(len(coordinates)):\n",
    "        coord = coordinates[i]\n",
    "\n",
    "        x1 = int(coord[0])\n",
    "        y1 = int(coord[1])\n",
    "        x2 = x1 + int(coord[2])\n",
    "        y2 = y1 + int(coord[3])\n",
    "\n",
    "        image = cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "# Model Name\n",
    "MODEL_NAME = 'ssd_mobilenet_v2_coco_2018_03_29'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_FROZEN_GRAPH = MODEL_NAME+\"/frozen_inference_graph.pb\"\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = \"models/research/object_detection/data/mscoco_label_map.pbtxt\"\n",
    "\n",
    "# Load graph\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to video and output\n",
    "DATA_FOLDER = Path(\"data\")\n",
    "\n",
    "# Video size does not affect object detection time\n",
    "width  = int(original_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(original_video.get(cv2.CAP_PROP_FRAME_HEIGHT)) \n",
    "\n",
    "\n",
    "input_video_filename = 'test_video.mp4'\n",
    "output_video_filename = 'output.avi'\n",
    "\n",
    "input_video_filepath = str(DATA_FOLDER/input_video_filename)\n",
    "output_video_filepath = str(DATA_FOLDER/output_video_filename)\n",
    "print(\"Input   - \",input_video_filepath)\n",
    "print(\"Output - \",output_video_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = cv2.VideoCapture(input_video_filepath)\n",
    "fps = input_video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter(output_video_filepath, fourcc, fps, (width, height))\n",
    "\n",
    "\n",
    "time_taken = {\"video_to_images\" : 0, \"run_inference\" : 0, \"filter_boxes\":0,\n",
    "              \"calc_coords\" : 0, \"draw_rects\" : 0, \"write\" : 0}\n",
    "with tqdm(total=int(input_video.get(cv2.CAP_PROP_FRAME_COUNT))) as pbar:\n",
    "    start = time()\n",
    "    for image in video_to_images(input_video):\n",
    "        time_taken[\"video_to_images\"] += time()-start\n",
    "        \n",
    "        # Actual detection\n",
    "        start = time()\n",
    "        output_dict = run_inference_for_single_image(image, detection_graph)\n",
    "        time_taken[\"run_inference\"] += time()-start\n",
    "        \n",
    "        start = time()\n",
    "        confidence_cutoff = 0.3\n",
    "        boxes, scores, classes = filter_boxes(confidence_cutoff, output_dict['detection_boxes'], \n",
    "            output_dict['detection_scores'], \n",
    "            output_dict['detection_classes'], [1])\n",
    "        time_taken[\"filter_boxes\"] += time()-start\n",
    "        \n",
    "        start = time()\n",
    "        # Tuples of (x, y) coords\n",
    "        centroids = []\n",
    "        # Lists of  [x, y, width, height]\n",
    "        coordinates = []\n",
    "        for box in boxes:\n",
    "            coord = calculate_coord(box, width, height)\n",
    "            centr = calculate_centr(coord)\n",
    "            centroids.append(centr)\n",
    "            coordinates.append(coord)\n",
    "        time_taken[\"calc_coords\"] += time()-start\n",
    "            \n",
    "        start = time()\n",
    "        # Draw rects on images\n",
    "        image = draw_rects(image, coordinates)\n",
    "        time_taken[\"draw_rects\"] += time()-start\n",
    "        # Centroid points\n",
    "\n",
    "        # Distance measuring??\n",
    "        # Draw lines??\n",
    "\n",
    "        # Write to output video file\n",
    "        start = time()\n",
    "        output_video.write(image)\n",
    "        pbar.update(1)\n",
    "        time_taken[\"write\"] += time()-start\n",
    "        \n",
    "        start = time()\n",
    "    \n",
    "output_video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For extracting one frame from video for testing\n",
    "original_video = cv2.VideoCapture('data/test_video.mp4')\n",
    "success,curr_image = original_video.read()\n",
    "cv2.imwrite(\"data/first_image.jpg\", curr_image)\n",
    "original_video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"data/first_image.jpg\")\n",
    "print(image.shape)\n",
    "# Expanding dimensions \n",
    "# Since the model expects images to have shape: [1, None, None, 3]\n",
    "image_expanded = np.expand_dims(image, axis=0)\n",
    "print(image.shape)\n",
    "\n",
    "# Actual detection.\n",
    "output_dict = run_inference_for_single_image(image, detection_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get boxes only for person\n",
    "confidence_cutoff = 0.3\n",
    "boxes, scores, classes = filter_boxes(confidence_cutoff, output_dict['detection_boxes'], \n",
    "output_dict['detection_scores'], \n",
    "output_dict['detection_classes'], [1])\n",
    "# Relative coordinates\n",
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = image.shape[1], image.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = []\n",
    "coordinates = []\n",
    "for box in boxes:\n",
    "    coord = calculate_coord(box, width, height)\n",
    "    centr = calculate_centr(coord)\n",
    "    centroids.append(centr)\n",
    "    coordinates.append(coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image = cv2.imread(\"data/first_image.jpg\")\n",
    "\n",
    "draw_rects(image, coordinates)\n",
    "show_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
